# -*- coding: utf-8 -*-
"""neural_network_raw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11nrNxzmuJ6ESVATWeBAyoXHbfRJXs1A_

# Neural Network Example

## Importing the libraries
"""

from __future__ import absolute_import, division, print_function

import tensorflow as tf
tf.enable_eager_execution()
import matplotlib.pyplot as plt
import numpy as np

"""## Preprocessing the dataset

### Loading the MNIST dataset from Keras datasets
"""

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train.shape, X_test.shape

"""### Converting the data from `int` to `float`"""

X_train, X_test = np.array(X_train, np.float32), np.array(X_test, np.float32)

"""### Flattening images to 1-D vector of 784 features (28*28)"""

X_train = X_train.reshape(-1, 28*28)
X_test = X_test.reshape(-1, 28*28)
X_train.shape, X_test.shape

"""### Normalizing values to the range of [0, 1]"""

X_train = X_train / 255.0
X_test = X_test / 255.0

"""### Using tf.data API to shuffle and batch data"""

batch_size = 32

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size)
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)

"""## Training a neural network model

### Parameters of the neural network
"""

n_input = 28*28
n_hidden_1 = 128
n_output = 10

"""### Initializing weights and biases for the neural network"""

weights = {
    'hidden_1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    'output': tf.Variable(tf.random_normal([n_hidden_1, n_output]))
    }

biases = {
    'hidden_1': tf.Variable(tf.random_normal([n_hidden_1])),
    'output': tf.Variable(tf.random_normal([n_output]))
    }

"""### Creating the neural network"""

def neural_network(x, is_training=True):
  # Hidden fully connected layer with n_hidden_1 neurons
  layer_1 = tf.add(tf.matmul(x, weights['hidden_1']), biases['hidden_1'])
  # Appling relu to layer_1 output for non-linearity
  layer_1 = tf.nn.relu(layer_1)
  # Appling dropout to layer_1 for regularization
  if is_training:
    layer_1 = tf.nn.dropout(layer_1, rate=0.2)
  # Output layer with a neuron for each classes
  output = tf.add(tf.matmul(layer_1, weights['output']), biases['output'])
  # Appling softmax to normalize the logits
  return tf.nn.softmax(output)

"""### Defining loss function and optimizer"""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

"""### Updating variables during training step and evaluating during testing step"""

def train_step(x_train, y_train):
  with tf.GradientTape() as g:
    y_pred = neural_network(x_train)
    loss = loss_object(y_train, y_pred)

  trainable_variables = list(weights.values()) + list(biases.values())
  gradients = g.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(gradients, trainable_variables))

  train_loss(loss)
  train_accuracy(y_train, y_pred)

def test_step(x_test, y_test):
  y_pred = neural_network(x_test, is_training=False)
  loss = loss_object(y_test, y_pred)

  test_loss(loss)
  test_accuracy(y_test, y_pred)

"""### Fitting the model on the training set and evaluating on the test set"""

EPOCHS = 10

for epoch in range(EPOCHS):
  for x_train, y_train in train_ds:
    train_step(x_train, y_train)

  for x_test, y_test in test_ds:
    test_step(x_test, y_test)

  print('Epoch {}: Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'.format(
      epoch + 1,
      train_loss.result(),
      train_accuracy.result() * 100,
      test_loss.result(),
      test_accuracy.result() * 100
  ))

  train_loss.reset_states()
  train_accuracy.reset_states()
  test_loss.reset_states()
  test_accuracy.reset_states()

"""### Visualizing the prediction of the trained model"""

# Visualize prediction
n_images = 10
test_images = X_test[:n_images]
predictions = neural_network(test_images, is_training=False)

# Display image and model prediction
for i in range(n_images):
    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')
    plt.show()
    print('Model prediction: {}'.format(np.argmax(predictions[i])))